import cv2
import matplotlib.pyplot as plt
import mediapipe as mp
from scipy.signal import find_peaks
import numpy as np
from mediapipe.framework.formats import landmark_pb2
from matplotlib.collections import LineCollection
from matplotlib.colors import LinearSegmentedColormap

BaseOptions = mp.tasks.BaseOptions
PoseLandmarker = mp.tasks.vision.PoseLandmarker
PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions
VisionRunningMode = mp.tasks.vision.RunningMode

# Video file 
videoFileName = '4-4stacatto(3).mp4'
videoOutputFileName = videoFileName + '_Full_output'
plotName = videoFileName + '_Full_coordinates_plot'
oordinatesPlotName = videoFileName + '_gradient.png'
swayPlotName = videoFileName + '_Full_Sway_plot'
rockingPlotName = videoFileName + '_Rocking_plot'
handsPlotName_X = videoFileName + '_Full_Hands_plot_X'
handsPlotName_Y = videoFileName + '_Full_Hands_plot_Y'
coordinatesPlotName = videoFileName + '_Full_coordinates_plot'

# Function to draw landmarks on the image
def draw_landmarks_on_image(rgb_image, detection_result):
    pose_landmarks_list = detection_result.pose_landmarks
    annotated_image = np.copy(rgb_image)

    for idx in range(len(pose_landmarks_list)):
        pose_landmarks = pose_landmarks_list[idx]

        # Draw the pose landmarks.
        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
        pose_landmarks_proto.landmark.extend([
            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks
        ])
        mp.solutions.drawing_utils.draw_landmarks(
            annotated_image,
            pose_landmarks_proto,
            mp.solutions.pose.POSE_CONNECTIONS,
            mp.solutions.drawing_styles.get_default_pose_landmarks_style()
        )
    return annotated_image

# Create a PoseLandmarker object
options = PoseLandmarkerOptions(
    base_options=BaseOptions(model_asset_path='pose_landmarker_lite.task'),
    running_mode=VisionRunningMode.VIDEO
)

# Create and initialize the PoseLandmarker
detector = PoseLandmarker.create_from_options(options)

# Arrays to store coordinates
frame_array = []  # Stores coordinates for the entire video
processed_frame_array = []  # Stores coordinates only during active processing intervals
processing_intervals = []  # List to store multiple processing intervals

frame_number = 0
processing_active = False
current_start_frame = None

# Text properties for beat display
font = cv2.FONT_HERSHEY_SIMPLEX
font_scale = 2
font_thickness = 2
font_color = (255, 255, 255)
text_display_duration = 3  # Number of frames to display "Beat!" text
text_display_counter = 0

# For video input:
cap = cv2.VideoCapture(videoFileName)

if not cap.isOpened():
    print("Error: Could not open video file.")
    exit()

# Initialize variables for beat detection and BPM calculation
bpm_window = 30  # Time window in seconds for BPM calculation
fps = int(cap.get(cv2.CAP_PROP_FPS))
frames_per_window = bpm_window * fps
beats = []  # List to store frames where beats occur

# Initialize flags and variables
processing_active = False
start_frame = None
end_frame = None
frame_number = 0

# Initialize VideoWriter for output video
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))
out = cv2.VideoWriter(videoOutputFileName + '.avi', cv2.VideoWriter_fourcc(*'MJPG'), fps, (frame_width, frame_height))

# Swaying variables
default_midpoint_x = 0
default_midpoint_z = 0
sway_threshold = 0.025  # Threshold for swaying detection
rocking_threshold = 0.05  # Threshold for swaying detection
midpoints_x = []
midpoints_z = []
midpointflag = False

# Lists to store hand coordinates
left_hand_x = []
left_hand_y = []
right_hand_x = []
right_hand_y = []

# First loop: Perform calculations
while cap.isOpened():
    success, image = cap.read()
    if not success:
        if processing_active:
            processing_intervals.append((current_start_frame, frame_number - 1))
        break

    frame_timestamp_ms = round(cap.get(cv2.CAP_PROP_POS_MSEC))
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)
    detection_result = detector.detect_for_video(mp_image, frame_timestamp_ms)
    annotated_image = draw_landmarks_on_image(image_rgb, detection_result)
    annotated_image_bgr = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)

    # Display annotated frame in the first loop
    cv2.imshow('Video Feed - Selection Mode', annotated_image_bgr)

    pose_landmarks_list = detection_result.pose_landmarks
    if pose_landmarks_list:
        for landmarks in pose_landmarks_list:
            if len(landmarks) > 16:
                x16 = landmarks[16].x
                y16 = landmarks[16].y
                frame_array.append((x16, y16))  # Add to main array for all frames
                # Filter out inactive frames with nan so they will be ignored by find_peaks
                processed_frame_array.append((np.nan, np.nan))
    
    # If processing is active, update the processed frame array
    if processing_active:
        pose_landmarks_list = detection_result.pose_landmarks
        if pose_landmarks_list:
            for landmarks in pose_landmarks_list:
                if len(landmarks) > 16:
                    x16 = landmarks[16].x
                    y16 = landmarks[16].y
                    # Update the processed frame array at the current frame index
                    if frame_number < len(processed_frame_array):
                        processed_frame_array[frame_number] = (x16, y16)
                    
                    # Calculate midpoints and movement
                    x12, x11 = landmarks[12].x, landmarks[11].x
                    z12, z11 = landmarks[12].z, landmarks[11].z
                        
                    midpoint_x = abs(x12 - x11) * 0.5 + x12
                    midpoint_z = z12
                        
                    midpoints_x.append(midpoint_x)
                    midpoints_z.append(midpoint_z)
                        
                    # Track hand movements
                    left_hand_x.append(landmarks[15].x)
                    left_hand_y.append(landmarks[15].y)
                    right_hand_x.append(landmarks[16].x)
                    right_hand_y.append(landmarks[16].y)

                    if midpointflag:
                        default_midpoint_x = midpoint_x
                        default_midpoint_z = midpoint_z
                        midpointflag = False

    # Display the current frame number
    cv2.putText(image, f'Frame: {frame_number}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)

    # Write the annotated frame to the output video
    out.write(annotated_image_bgr)

    # Check for user input to start/stop processing
    key = cv2.waitKey(5) & 0xFF
    if key == ord('s'):  # Press 's' to mark the start frame
        if not processing_active:
            current_start_frame = frame_number
            # start_frame = frame_number
            midpointflag = True
            processing_active = True
            print(f"Processing started at frame {current_start_frame}")
    elif key == ord('e'):  # Press 'e' to mark the end frame
        if processing_active:
            # end_frame = frame_number
            midpointflag = False
            processing_active = False
            print(f"Processing stopped at frame {frame_number}")
            processing_intervals.append((current_start_frame, frame_number))
    elif key == 27:  # Press 'ESC' to exit
        if processing_active:
            processing_intervals.append((current_start_frame, frame_number))
        break

    frame_number += 1

cap.release()
out.release()
cv2.destroyAllWindows()

print("Processing intervals:", processing_intervals)

detector = PoseLandmarker.create_from_options(options)

x = [coord[0] for coord in frame_array]
y = [coord[1] for coord in frame_array]

# Find peaks and valleys in x and y coordinates
x_peaks, _ = find_peaks(x)
x_valleys, _ = find_peaks([-val for val in x])
y_peaks, _ = find_peaks(y)
y_valleys, _ = find_peaks([-val for val in y])

# Convert x_proc and y_proc to numpy arrays for proper arithmetic operations
x_proc = np.array([coord[0] for coord in processed_frame_array])
y_proc = np.array([coord[1] for coord in processed_frame_array])

# Directly apply find_peaks to x_proc and y_proc as numpy arrays
x_peaks_proc, _ = find_peaks(x_proc)
x_valleys_proc, _ = find_peaks(-x_proc)  # Now valid with numpy arrays
y_peaks_proc, _ = find_peaks(y_proc)
y_valleys_proc, _ = find_peaks(-y_proc)

# Ensure each peaks/valleys array is a list of indices
x_peaks_proc = list(x_peaks_proc)
x_valleys_proc = list(x_valleys_proc)
y_peaks_proc = list(y_peaks_proc)
y_valleys_proc = list(y_valleys_proc)

threshold = 10
#Threshold to see when peaks and valleys are going to be part of the same "beat"
proximity_threshold = 5
def filter_significant_points(x_peaks_proc, x_valleys_proc, y_peaks_proc, y_valleys_proc, threshold, proximity_threshold):
    # Combine all points but keep track of which are Y points
    all_points = []
    for frame in x_peaks_proc:
        all_points.append((frame, False))  # False indicates not a Y point
    for frame in x_valleys_proc:
        all_points.append((frame, False))
    for frame in y_peaks_proc:
        all_points.append((frame, True))   # True indicates Y point
    for frame in y_valleys_proc:
        all_points.append((frame, True))
    
    # Sort by frame number
    all_points.sort(key=lambda x: x[0])
    
    filtered_points = []
    last_added_frame = -threshold
    
    i = 0
    while i < len(all_points):
        current_frame = all_points[i][0]
        
        # Skip if too close to last added beat
        if current_frame - last_added_frame < threshold:
            i += 1
            continue
        
        # Look ahead for nearby points
        nearby_points = []
        j = i
        while j < len(all_points) and all_points[j][0] - current_frame <= proximity_threshold:
            nearby_points.append(all_points[j])
            j += 1
        
        # Check if any nearby points are Y points
        y_points = [p for p in nearby_points if p[1]]  # p[1] is True for Y points
        
        if y_points:
            # Use the first Y point in the group
            filtered_points.append(y_points[0][0])
            last_added_frame = y_points[0][0]
        else:
            # No Y points nearby, use the first point in the group
            filtered_points.append(nearby_points[0][0])
            last_added_frame = nearby_points[0][0]
        
        i = j  # Skip past all nearby points
            
    return filtered_points

# Use the function
filtered_significant_beats = filter_significant_points(x_peaks_proc,  x_valleys_proc,  y_peaks_proc, y_valleys_proc, threshold, proximity_threshold)
frame_index = 0

# Initialize for second video capture loop
cap = cv2.VideoCapture(videoFileName)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
size = (frame_width, frame_height)
fps = int(cap.get(cv2.CAP_PROP_FPS))  

# Change codec if necessary
out = cv2.VideoWriter(videoOutputFileName + '.avi', cv2.VideoWriter_fourcc(*'MJPG'), fps, size)

# Initialize text properties
font = cv2.FONT_HERSHEY_SIMPLEX
font_scale = 2  
font_thickness = 2
font_color = (255, 255, 255)
text_display_duration = 3  
text_display_counter = 0

def is_within_intervals(frame_idx, intervals):
    return any(start <= frame_idx <= end for start, end in intervals)

# Function to calculate BPM based on beats within the last time window
def calculate_bpm(current_frame, beats, fps, window_duration):
    # Filter out beats that fall outside the time window
    beats_in_window = [beat for beat in beats if current_frame - beat <= window_duration * fps]
    bpm = len(beats_in_window) * (60 / window_duration)
    return bpm

# Second loop: Use stored results to annotate the output video
while cap.isOpened():
    success, image = cap.read()
    if not success:
        print("Ending processing.")
        break

    image_bgr = image

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_timestamp_ms = round(cap.get(cv2.CAP_PROP_POS_MSEC))

    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)

    detection_result = detector.detect_for_video(mp_image, frame_timestamp_ms)
    annotated_image = draw_landmarks_on_image(image_rgb, detection_result)
    annotated_image_bgr = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)

  # Check if current frame is within processing intervals and handle beat text
    if is_within_intervals(frame_index, processing_intervals):
        # Display "Beat!" text and calculate BPM if frame is a beat
        if frame_index in filtered_significant_beats:
            # Calculate text position and draw text
            text = "Beat!"
            text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]
            text_x = (annotated_image_bgr.shape[1] - text_size[0]) // 2
            text_y = (annotated_image_bgr.shape[0] + text_size[1]) // 2
            cv2.putText(annotated_image_bgr, text, (text_x, text_y), font, font_scale, font_color, font_thickness)

            # Record the current beat and calculate BPM
            beats.append(frame_index)
            bpm = calculate_bpm(frame_index, beats, fps, bpm_window)

            # Save BPM information to file
            bpm_info = f'Beats per minute (BPM) at frame {frame_index}: {bpm}\n'
            output_file = '4-4stacatto(2).mp4_auto_BPM.txt'
            with open(output_file, 'a') as file:
                print(bpm_info, end='')
                file.write(bpm_info)

            # Reset display counter for beat text duration
            text_display_counter = text_display_duration
        else:
            # Only display "Beat!" text if counter is active
            if text_display_counter > 0:
                cv2.putText(annotated_image_bgr, "Beat!", (text_x, text_y), font, font_scale, font_color, font_thickness)
                text_display_counter -= 1  # Decrement counter

    # Swaying
    if frame_index < len(midpoints_x):
        midpoint_x = midpoints_x[frame_index]
        if midpoint_x > default_midpoint_x + sway_threshold or midpoint_x < default_midpoint_x - sway_threshold:
            cv2.putText(annotated_image_bgr, "Swaying", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)

    # Rocking
    if frame_index < len(midpoints_z):
        midpoint_z = midpoints_z[frame_index]
        if midpoint_z > default_midpoint_z + rocking_threshold or midpoint_z < default_midpoint_z - rocking_threshold:
            cv2.putText(annotated_image_bgr, "Rocking", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)

    cv2.putText(annotated_image_bgr, f'Frame: {frame_index}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)
    cv2.imshow('Annotated Frames', annotated_image_bgr)
    out.write(annotated_image_bgr)

    key = cv2.waitKey(5) & 0xFF
    if key == 27:
        break

    frame_index += 1

cap.release()
out.release()
cv2.destroyAllWindows()

# Plotting x and y coordinates with uniform beat markers within processing intervals
plt.figure(figsize=(12, 6))
plt.plot(range(len(x)), x, label='X Coordinates', color='b', alpha=0.7)
plt.plot(range(len(y)), y, label='Y Coordinates', color='g', alpha=0.7)

# Highlight each processing interval individually on the plot
if processing_intervals:  # Ensure there are intervals to process
    for start, end in processing_intervals:
        plt.axvspan(start, end, color='yellow', alpha=0.3, label="Processed Range" if start == processing_intervals[0][0] else None)

# Plot uniform beat markers for all filtered peaks and valleys
all_beats = sorted(filtered_significant_beats)
all_beat_values = [x[i] if i < len(x) else y[i - len(x)] for i in all_beats]
# Plot vertical lines for all filtered peaks and valleys
for beat in all_beats:
    plt.axvline(x=beat, color='purple', linestyle='--', label="Beats" if beat == all_beats[0] else None)
plt.plot(x_valleys, [x[i] for i in x_valleys], "x", label="X Valleys")
plt.plot(y_peaks, [y[i] for i in y_peaks], "o", label="Y Peaks")
plt.plot(y_valleys, [y[i] for i in y_valleys], "o", label="Y Valleys")
plt.title('X and Y Coordinates Over Frame Number')
plt.xlabel('Frame Number')
plt.ylabel('Coordinate Value')
plt.legend()
plt.grid(True)
plt.savefig(coordinatesPlotName +'.png')
plt.show()

# Create the figure
plt.figure(figsize=(12, 6))

# Filter out NaN values and create valid points
valid_mask = ~(np.isnan(x_proc) | np.isnan(y_proc))
x_valid = x_proc[valid_mask]
y_valid = y_proc[valid_mask]

# Invert the Y coordinates to match natural movement
y_valid = -y_valid  

# Create points for the line
points = np.array([x_valid, y_valid]).T.reshape(-1, 1, 2)
segments = np.concatenate([points[:-1], points[1:]], axis=1)

# Create a custom colormap from blue to purple
colors = ['blue', 'purple']
n_bins = len(x_valid)
custom_cmap = LinearSegmentedColormap.from_list("custom", colors, N=n_bins)

# Create an array of values for coloring
norm = plt.Normalize(0, len(x_valid))
lc = LineCollection(segments, cmap=custom_cmap, norm=norm)
lc.set_array(np.arange(len(x_valid)))

# Create the plot
plt.gca().add_collection(lc)

# Set axis limits using valid data points
plt.xlim(np.nanmin(x_valid), np.nanmax(x_valid))
plt.ylim(np.nanmin(y_valid), np.nanmax(y_valid))

# Add colorbar to show the gradient
cbar = plt.colorbar(lc)
cbar.set_label('Frame Number')

# Add labels and title
plt.xlabel("X-Coords")
plt.ylabel("Y-Coords")
plt.title("Conducting Pattern")

# Add grid for better readability
plt.grid(True, linestyle='--', alpha=0.7)

# Save and show the plot
plt.savefig(coordinatesPlotName + '_clean_pattern.png', bbox_inches='tight')
plt.show()

# Plot the swaying detection graph
plt.figure(figsize=(12, 6))
plt.plot(range(len(midpoints_x)), midpoints_x, label='Current Midpoint X', color='b', alpha=0.7)
plt.axhline(y=default_midpoint_x, color='k', linestyle='-', label='Default Midpoint X')
plt.axhline(y=default_midpoint_x + sway_threshold, color='r', linestyle='--', label='Upper Threshold X')
plt.axhline(y=default_midpoint_x - sway_threshold, color='r', linestyle='--', label='Lower Threshold X')
plt.title('Swaying Detection Over Frame Number')
plt.xlabel('Frame Number')
plt.ylabel('Midpoint X Value')
plt.legend()
plt.grid(True)
plt.savefig(swayPlotName + '.png')
plt.show()

# Plot the rocking detection graph
plt.figure(figsize=(12, 6))
plt.plot(range(len(midpoints_z)), midpoints_z, label='Current Midpoint Z', color='b', alpha=0.7)
plt.axhline(y=default_midpoint_z, color='k', linestyle='-', label='Default Midpoint Z')
plt.axhline(y=default_midpoint_z + rocking_threshold, color='r', linestyle='--', label='Upper Threshold Z')
plt.axhline(y=default_midpoint_z - rocking_threshold, color='r', linestyle='--', label='Lower Threshold Z')
plt.title('Rocking Detection Over Frame Number')
plt.xlabel('Frame Number')
plt.ylabel('Midpoint Z Value')
plt.legend()
plt.grid(True)
plt.savefig(rockingPlotName + '.png')
plt.show()

# Plot the hand coordinates graph x 
plt.figure(figsize=(12, 6))
plt.plot(range(len(left_hand_x)), [x - left_hand_x[0] for x in left_hand_x], label='Left Hand X', color='b', alpha=0.7)
plt.plot(range(len(right_hand_x)), [x - right_hand_x[0] for x in right_hand_x], label='Right Hand X', color='g', alpha=0.7)
plt.axhline(y=0, color='k', linestyle='-', label='Default Line')
plt.title('Hands X Coordinates Over Frame Number')
plt.xlabel('Frame Number')
plt.ylabel('Coordinate Value')
plt.legend()
plt.grid(True)
plt.savefig(handsPlotName_X + '.png')
plt.show()

# Plot the hand coordinates graph y 
plt.figure(figsize=(12, 6))
plt.plot(range(len(left_hand_y)), [y - left_hand_y[0] for y in left_hand_y], label='Left Hand Y', color='r', alpha=0.7)
plt.plot(range(len(right_hand_y)), [y - right_hand_y[0] for y in right_hand_y], label='Right Hand Y', color='m', alpha=0.7)
plt.axhline(y=0, color='k', linestyle='-', label='Default Line')
plt.title('Hands Y Coordinates Over Frame Number')
plt.xlabel('Frame Number')
plt.ylabel('Coordinate Value')
plt.legend()
plt.grid(True)
plt.savefig(handsPlotName_Y + '.png')
plt.show()

cv2.destroyAllWindows()
